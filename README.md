# Project Title

Sonic Textual Analysis: Exploring Diaries through Data Sonification

# Overview

This project explores a novel approach to textual analysis by converting textual data (specifically from diaries sourced from prozhito.org) into musical soundscapes (data sonification). The subsequent analysis of this generated music aims to uncover deeper cognitive insights and emotional nuances within the original texts.

The process is iterative, following the concept of a "hermeneutic circle": text analysis informs music generation, and the analysis of that music, in turn, enriches the understanding of the text, creating a cycle of interpretation.

Key analytical methods used on the text include sentiment analysis (utilizing the Dostoevsky library for Russian texts) and topic modeling. The primary tools for sonification are Sonic Pi and SuperCollider.

This project is part of Ph.D. research into using music as a means of cognition.

## Project Structure

This project is organized into several directories:

*   `1_fetching_data/`: Contains Node.js scripts for acquiring text data (e.g., from prozhito.org).
*   `2_analitics/`: Houses Python scripts for initial text analysis, such as generating word clouds and performing topic modeling.
*   `audio/`: Stores example audio files generated through the sonification process.
*   `csv/`: Contains various CSV files that are either generated by data processing scripts (e.g., `22Data.csv`, `22DataTone.csv`) or used as input for sonification.
*   `dostoevsky/`: Includes the Dostoevsky library (as a submodule or downloaded data) for sentiment analysis of Russian texts.
*   `html/`: Stores HTML output files, which might include visualizations or reports.
*   `img/`: Contains image files, such as plots generated from text analysis (e.g., sentiment trends, topic distributions) and word clouds.
*   `scrapy/`: Contains Python Scrapy spiders for web scraping tasks, likely used for gathering data from specific websites like prozhito.org.
*   `sonification/`: This directory is central to the sonification process and includes:
    *   `sonicPi/`: Scripts and text files for generating music using Sonic Pi.
    *   `superCollider/`: Code and configurations for creating sound with SuperCollider.
    *   `midi_time/`: Scripts related to MIDI generation or processing.
*   `sonification_react_network_client/`: Contains the source code for a separate React-based frontend application designed to interact with this project's outputs or API.
*   `tone_value/`: Includes Python scripts focused on detailed sentiment/tone analysis, normalizing data, and preparing it for the sonification stage.
*   `requirements.txt`: Lists the Python dependencies for the project.
*   `.ipynb_checkpoints/`: While typically for Jupyter Notebook checkpoints, this directory in the project also contains some analysis notebooks. (It's advisable to move important notebooks to a dedicated 'notebooks' directory for better organization).

## Prerequisites and Installation

Before you begin, ensure you have the following software installed:

*   **Python 3.x:** Most scripts are written in Python. You can download it from [python.org](https://www.python.org/).
*   **Node.js and npm:** Required for the data fetching scripts. You can download them from [nodejs.org](https://nodejs.org/).
*   **Git:** For cloning the repository and managing submodules.
*   **Sonic Pi:** (Optional) If you want to run the Sonic Pi sonification scripts. Download from [sonic-pi.net](https://sonic-pi.net/).
*   **SuperCollider:** (Optional) If you want to run the SuperCollider sonification scripts. Download from [supercollider.github.io](https://supercollider.github.io/).

Follow these steps to set up the project:

1.  **Clone the repository (if you haven't already):**
    If you're viewing this README on GitHub, you've likely already done this or can download the source. If not:
    ```bash
    git clone https://github.com/yarov475/sonicDataAnalizeYaro.git # Assuming this is the repo
    cd sonicDataAnalizeYaro
    ```

2.  **Install Python dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

3.  **Download Dostoevsky sentiment model:**
    The project includes the Dostoevsky library files directly. To use it, download its pre-trained model:
    ```bash
    python3 -m dostoevsky download fasttext-social-network-model
    ```
    (This will typically download the model to a standard location expected by the Dostoevsky library, e.g., inside the `dostoevsky` package directory or a user's application data folder.)

4.  **Set up data fetching (if needed):**
    Navigate to the `1_fetching_data/` directory:
    ```bash
    cd 1_fetching_data/
    npm install
    cd .. 
    ```
    (This installs Node.js dependencies for the data fetching scripts.)

## Workflow and Usage

This project involves several stages, from data acquisition to analysis and sonification.

**Important Note on Paths:** Unless specified, Python scripts are generally expected to be run from the project's root directory. Adjust paths in commands if you run them from elsewhere.

**Step 1: Data Fetching**

The primary data (diaries) seems to be processed into `22Data.csv`.
1.  Navigate to the data fetching directory:
    ```bash
    cd 1_fetching_data/
    ```
2.  Ensure Node.js dependencies are installed (as per the Installation section):
    ```bash
    npm install 
    ```
3.  Run the fetching script (the old README mentioned `node fetchTo22dataCSV.js` and also `node fetchData`. Assuming `fetchTo22dataCSV.js` is the correct one as it's more specific):
    ```bash
    node fetchTo22dataCSV.js
    ```
    This should generate or update the `csv/22Data.csv` file.
4.  Return to the project root directory:
    ```bash
    cd ..
    ```

**Step 2: Text Analysis**

These scripts perform initial analysis on the fetched text data.
1.  **Word Cloud Generation:**
    To visualize the most frequent words in the dataset:
    ```bash
    python 2_analitics/text22Data_analitics.py
    ```
    This will likely save a word cloud image in the `img/` directory (e.g., `img/world_cloud22data_text.png`).

2.  **Topic Modeling:**
    To identify underlying topics in the texts:
    ```bash
    python 2_analitics/topic_model.py
    ```
    Outputs from this script (like `topic_model.txt` mentioned in the old README) will be used in subsequent steps.

**Step 3: Sentiment and Tone Processing for Sonification**

This stage involves scripts primarily from the `tone_value/` directory to process text for sentiment/tone and prepare it for sonification. The exact sequence and interplay of these scripts might require deeper inspection of their code, but based on the old README:

1.  **Normalize Topic Data:**
    ```bash
    python tone_value/normalize_topic_data1.py 
    ```
    (This likely processes outputs from the topic modeling.)

2.  **Prepare Data for Sound (General):**
    ```bash
    python tone_value/data_to_sound.py
    ```

3.  **Tone Variable Creation / Sentiment Analysis:**
    ```bash
    python tone_value/tone.py 
    ```
    (The old README mentioned "#create var", suggesting this script sets up or calculates tone/sentiment values.)

4.  **Create Tone-Enriched CSV:**
    ```bash
    python tone_value/22DataToneCsv_creater.py
    ```
    This script is expected to produce `csv/22DataTone.csv`.

5.  **Generate Plots:**
    Visualize the sentiment/tone data:
    ```bash
    python tone_value/text_tone_plot_creater.py
    python tone_value/topic_tone_plot_creater.py
    ```
    These scripts should generate plots like `img/text.png` and `img/topic.png`.

**Step 4: Sonification**

This step uses the processed data to generate sound.

1.  **Using SuperCollider:**
    *   The project generates CSV files like `csv/tone_topic_data_to_sc.csv` or uses data from other CSVs.
    *   Open the SuperCollider scripts located in `sonification/superCollider/` (e.g., `22.scd`, `ful_data_22_algo.scd`).
    *   These scripts will likely load the CSV data and use it to generate sound. You'll need to run the scripts within the SuperCollider environment.

2.  **Using Sonic Pi:**
    *   The file `sonification/sonicPi/adsSonicPiMusic.txt` contains Sonic Pi code.
    *   Copy the content of this file into the Sonic Pi application and run it. It might be designed to read data from one of the generated CSV files.

3.  **MIDI Generation:**
    To create a MIDI file from the data:
    ```bash
    python sonification/midi_time/midiTime.py
    ```
    This is expected to produce a `.mid` file (e.g., `22midi.mid` in `sonification/midi_time/`).

**Step 5: Music Analysis (Example)**

The project includes an example of how the generated music can be analyzed.
*   A Jupyter Notebook, likely `plot_22_full.ipynb` (which was found in `.ipynb_checkpoints/`), demonstrates analysis using libraries like Librosa.
*   Ideally, move this notebook to a more accessible directory like `notebooks/`.
*   Running this notebook will show steps like loading an audio file (e.g., `audio/22_full_data.wav`) and performing spectral analysis, potentially generating images like `msc.png`.

## Outputs and External Links

Running the various scripts and processes in this project will generate several types of outputs:

*   **CSV Files:** Located in the `csv/` directory, these files contain structured data, including the initial texts (`22Data.csv`), sentiment and topic information (`22DataTone.csv`, `tone_topic_data.csv`), and data formatted for sonification tools.
*   **Plots and Images:** Found in the `img/` directory, these include:
    *   Word clouds (`world_cloud22data_text.png`)
    *   Sentiment trend plots (`text.png`, `topic.png`)
    *   Spectral analysis images from music analysis (`msc.png`)
*   **Audio Files:** Sample audio outputs from the sonification process are stored in the `audio/` directory (e.g., `22_full_data.wav`, `keywordMusic.wav`).
*   **MIDI Files:** Generated MIDI data (e.g., `22midi.mid`) can be found in `sonification/midi_time/`.
*   **HTML Files:** Some processes might generate HTML reports or visualizations, stored in `html/`.

**External Links:**

*   **Author's SoundCloud:** You can listen to examples of music generated by this project and related research on Yaroslav's SoundCloud: [https://soundcloud.com/yarochkin_sonic](https://soundcloud.com/yarochkin_sonic)
*   **Client Application:** A separate React-based client application has been developed to interact with this project's data or concepts. You can find its repository here: [https://github.com/yarov475/sonicDataAnalizeYaro_client_prozhito_api](https://github.com/yarov475/sonicDataAnalizeYaro_client_prozhito_api)

## Research Context

This project is a component of ongoing Ph.D. research focused on exploring music as a means of cognition and understanding complex data, particularly in the domain of textual analysis. The methodologies and tools developed here contribute to investigating how sonification can aid in uncovering nuanced insights from textual sources like personal diaries.
